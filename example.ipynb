{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage\n",
    "\n",
    "This is a basic example using the torchvision COCO dataset from coco.py, it assumes that you've already downloaded the COCO images and annotations JSON.  You'll notice that the scale augmentations are quite extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from copy_paste import CopyPaste\n",
    "from coco import CocoDetectionCP\n",
    "from visualize import display_instances\n",
    "import albumentations as A\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy_paste import copy_paste_class\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "@copy_paste_class\n",
    "class FigaroDataset(Dataset):\n",
    "    def __init__(self, transforms=None):\n",
    "        # super(FigaroDataset, self).__init__(*args)\n",
    "        self.impath = glob.glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/src/*.jpg')\n",
    "        self.maskpath = glob.glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/mask/*.pbm')\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.impath)\n",
    "\n",
    "    def load_example(self, idx):\n",
    "        path = self.impath[idx]\n",
    "        mask_path = self.maskpath[idx]\n",
    "        \n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        class_id = 1\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin,ymin,xmax,ymax,class_id])\n",
    "            boxes.append([xmin,ymin,xmax,ymax,class_id])\n",
    "\n",
    "        # print(masks.shape)\n",
    "        # masks = [masks]\n",
    "        masks = [masks.squeeze(0).astype(np.uint8),masks.squeeze(0).astype(np.uint8)]\n",
    "        \n",
    "        output = {\n",
    "                    'image': image,\n",
    "                    'masks': masks,\n",
    "                    'bboxes': boxes,\n",
    "                }\n",
    "        \n",
    "        return self.transforms(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "def image_copy_paste(img, paste_img, alpha, blend=True, sigma=1):\n",
    "    if alpha is not None:\n",
    "        if blend:\n",
    "            alpha = gaussian(alpha, sigma=sigma, preserve_range=True)\n",
    "\n",
    "        img_dtype = img.dtype\n",
    "        alpha = alpha[..., None]\n",
    "        img = paste_img * alpha + img * (1 - alpha)\n",
    "        img = img.astype(img_dtype)\n",
    "\n",
    "    return img\n",
    "\n",
    "def mask_copy_paste(mask, paste_mask, alpha):\n",
    "    raise NotImplementedError\n",
    "\n",
    "def masks_copy_paste(masks, paste_masks, alpha):\n",
    "    if alpha is not None:\n",
    "        #eliminate pixels that will be pasted over\n",
    "        masks = [\n",
    "            np.logical_and(mask, np.logical_xor(mask, alpha)).astype(np.uint8) for mask in masks\n",
    "        ]\n",
    "        masks.extend(paste_masks)\n",
    "\n",
    "    return masks\n",
    "\n",
    "def extract_bboxes(masks):\n",
    "    bboxes = []\n",
    "    # allow for case of no masks\n",
    "    if len(masks) == 0:\n",
    "        return bboxes\n",
    "    \n",
    "    h, w = masks[0].shape\n",
    "    for mask in masks:\n",
    "        yindices = np.where(np.any(mask, axis=0))[0]\n",
    "        xindices = np.where(np.any(mask, axis=1))[0]\n",
    "        if yindices.shape[0]:\n",
    "            y1, y2 = yindices[[0, -1]]\n",
    "            x1, x2 = xindices[[0, -1]]\n",
    "            y2 += 1\n",
    "            x2 += 1\n",
    "            y1 /= w\n",
    "            y2 /= w\n",
    "            x1 /= h\n",
    "            x2 /= h\n",
    "        else:\n",
    "            y1, x1, y2, x2 = 0, 0, 0, 0\n",
    "\n",
    "        bboxes.append((y1, x1, y2, x2))\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "def bboxes_copy_paste(bboxes, paste_bboxes, masks, paste_masks, alpha, key):\n",
    "    if key == 'paste_bboxes':\n",
    "        return bboxes\n",
    "    elif paste_bboxes is not None:\n",
    "        masks = masks_copy_paste(masks, paste_masks=[], alpha=alpha)\n",
    "        adjusted_bboxes = extract_bboxes(masks)\n",
    "\n",
    "        #only keep the bounding boxes for objects listed in bboxes\n",
    "        mask_indices = [box[-1] for box in bboxes]\n",
    "        adjusted_bboxes = [adjusted_bboxes[idx] for idx in mask_indices]\n",
    "        #append bbox tails (classes, etc.)\n",
    "        adjusted_bboxes = [bbox + tail[4:] for bbox, tail in zip(adjusted_bboxes, bboxes)]\n",
    "\n",
    "        #adjust paste_bboxes mask indices to avoid overlap\n",
    "        if len(masks) > 0:\n",
    "            max_mask_index = len(masks)\n",
    "        else:\n",
    "            max_mask_index = 0\n",
    "\n",
    "        paste_mask_indices = [max_mask_index + ix for ix in range(len(paste_bboxes))]\n",
    "        paste_bboxes = [pbox[:-1] + (pmi,) for pbox, pmi in zip(paste_bboxes, paste_mask_indices)]\n",
    "        adjusted_paste_bboxes = extract_bboxes(paste_masks)\n",
    "        adjusted_paste_bboxes = [apbox + tail[4:] for apbox, tail in zip(adjusted_paste_bboxes, paste_bboxes)]\n",
    "\n",
    "        bboxes = adjusted_bboxes + adjusted_paste_bboxes\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgpath = glob.glob('/mnt/vitasoft/kobaco/dataset/data_processing/kobaco_data/scene/**/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_impath = glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/src/*.jpg')\n",
    "self_maskpath = glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/mask/*pbm')\n",
    "self_humanmaskpath = glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/humanmask/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impath = glob.glob('/home/ubuntu/workspace/CelebAMask-HQ/CelebA-HQ-img/*')\n",
    "# impath = glob.glob('/home/ubuntu/workspace/CelebAMask-HQ/CelebAMask-HQ-mask-anno/*')\n",
    "self_impath += glob('/home/ubuntu/workspace/CelebAMask-HQ/CelebA-HQ-img/*.jpg')\n",
    "self_maskpath += glob('/home/ubuntu/workspace/CelebAMask-HQ/mask/*.png')\n",
    "self_humanmaskpath += glob('/home/ubuntu/workspace/CelebAMask-HQ/humanmask/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(self_humanmaskpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "a[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rvm_model.model import MattingNetwork\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "model = torch.hub.load(\"PeterL1n/RobustVideoMatting\", \"mobilenetv3\") # or \"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "bgr = torch.tensor([.47, 1, .6]).view(3, 1, 1).cuda()\n",
    "downsample_ratio = 0.8                              # Adjust based on your video.\n",
    "\n",
    "device = 'cuda'\n",
    "def cv2_frame_to_cuda(frame):\n",
    "\t\"\"\"\n",
    "\tconvert cv2 frame to tensor.\n",
    "\t\"\"\"\n",
    "\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\tloader = ToTensor()\n",
    "\treturn loader(Image.fromarray(frame)).to(device,torch.float32,non_blocking=True).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = '/mnt/vitasoft/kobaco_batch/Video.frame/20211112223104/763848F02_1-18-TH-07-531/*'\n",
    "path = '/mnt/vitasoft/kobaco/sketchy/RobustVideoMatting/src/*.jpg'\n",
    "\n",
    "from glob import glob\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for item in glob(path):\n",
    "        cnt+=1\n",
    "    #     img = cv2.imread('../U-2-Net/test_img.jpg')\n",
    "        img = cv2.imread(item)\n",
    "        src = cv2_frame_to_cuda(img).cuda()\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        rec = [None] * 4\n",
    "        fgr, pha, *rec = model(src.cuda(), *rec, 0.4) \n",
    "        com = fgr * pha + 0 * (1 - pha)\n",
    "        com = com.mul(255).byte().cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "        com = cv2.cvtColor(com, cv2.COLOR_RGB2BGR)\n",
    "        im = Image.fromarray(com)\n",
    "        a = cv2.hconcat([img,com])\n",
    "        # Image.fromarray(a).show()\n",
    "        display(Image.fromarray(a))\n",
    "        # cv2.imwrite('seg/'+str(cnt)+'.jpg',a)\n",
    "        # display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for path in self_impath:\n",
    "    at_img = cv2.imread(path)\n",
    "    with torch.no_grad():\n",
    "        src = cv2_frame_to_cuda(at_img).cuda()\n",
    "        rec = [None] * 4\n",
    "        fgr, pha, *rec = model(src.cuda(), *rec, 0.4) \n",
    "\n",
    "        fgr[:,:,:,:] = 1\n",
    "        pha[pha>=0.5] = 1\n",
    "        pha[pha<0.5] = 0\n",
    "\n",
    "        com = fgr * pha + 0 * (1 - pha)\n",
    "        com = com.mul(255).byte().cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "        mask_ = cv2.cvtColor(com, cv2.COLOR_RGB2BGR)\n",
    "        display(Image.fromarray(mask_))\n",
    "        # cv2.imwrite('humanmask/'+os.path.basename(path)[:-4]+'.png', mask_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_impath =sorted(self_impath)\n",
    "# at_mask = cv2.imread(self_maskpath[at_idx])\n",
    "# at_humanmask = cv2.imread(self_humanmaskpath[at_idx])\n",
    "self_maskpath = sorted(self_maskpath)\n",
    "self_humanmaskpath = sorted(self_humanmaskpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "path = bgpath[200]\n",
    "cnt = 1\n",
    "for path in bgpath:\n",
    "\n",
    "    image = cv2.imread(path)\n",
    "    bg_h, bg_w = image.shape[:2]\n",
    "    mask = np.zeros((bg_h, bg_w))\n",
    "\n",
    "    ## random choice num of attach images\n",
    "    num_of_at = random.randrange(1, 5)\n",
    "    print(num_of_at)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    while(1):\n",
    "        if num_of_at == 0:\n",
    "            break\n",
    "\n",
    "        at_idx = random.randrange(0,len(self_impath))\n",
    "        print(self_impath[at_idx])\n",
    "        print(self_maskpath[at_idx])\n",
    "        at_img = cv2.imread(self_impath[at_idx])\n",
    "        at_mask = cv2.imread(self_maskpath[at_idx])\n",
    "        at_humanmask = cv2.imread(self_humanmaskpath[at_idx])\n",
    "        at_mask = cv2.cvtColor(at_mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ## random resize\n",
    "        h, w = at_img.shape[:2]\n",
    "        at_resize_factor = random.uniform(0.1, 1.0)\n",
    "        dsize = (int(w*at_resize_factor),int(h*at_resize_factor))\n",
    "        at_img = cv2.resize(at_img,dsize=dsize)\n",
    "        at_mask = cv2.resize(at_mask,dsize=dsize)\n",
    "        at_humanmask = cv2.resize(at_humanmask,dsize=dsize)\n",
    "\n",
    "        ## random locate selection\n",
    "        max_locate_h = bg_h-dsize[1]\n",
    "        max_locate_w = bg_w-dsize[0]\n",
    "\n",
    "        if max_locate_h <= 0 or max_locate_w <= 0:\n",
    "            continue\n",
    "\n",
    "        locate_h = random.randrange(0, max_locate_h)\n",
    "        locate_w = random.randrange(0, max_locate_w)\n",
    "\n",
    "        mask_ = cv2.cvtColor(at_humanmask, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        mask_inv = 255 - mask_\n",
    "        fg = cv2.bitwise_and(at_img, at_img, mask=mask_)\n",
    "        crop_image = image[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0],:]\n",
    "        bg = cv2.bitwise_and(crop_image, crop_image, mask=mask_inv)\n",
    "\n",
    "\n",
    "        image[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0],:] = fg+bg\n",
    "\n",
    "        crop_mask = mask[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0]]\n",
    "        crop_mask[at_mask==255] = 255\n",
    "\n",
    "        mask[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0]] = crop_mask\n",
    "        num_of_at -= 1\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=2)\n",
    "    mask_sq = np.squeeze(mask, axis=2).astype(np.uint8)\n",
    "    mask_sq = cv2.cvtColor(mask_sq, cv2.COLOR_GRAY2BGR)\n",
    "    # plt.imshow(mask_,)\n",
    "    # plt.show()\n",
    "    # display(Image.fromarray(mask_))\n",
    "    # display(Image.fromarray(image))\n",
    "    # cv2.imwrite('result/'+str(cnt)+'.png',cv2.hconcat([image,mask_sq]))\n",
    "    display(Image.fromarray(cv2.hconcat([image,mask_sq])))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_humanmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(at_img)\n",
    "# plt.show()\n",
    "# plt.imshow(at_mask, cmap='gray')\n",
    "# plt.show()\n",
    "#\n",
    "print(mask.shape)\n",
    "mask_sq = np.squeeze(mask, axis=2).astype(np.uint8)\n",
    "print(mask_sq.shape)\n",
    "mask_sq = cv2.cvtColor(mask_sq, cv2.COLOR_GRAY2BGR)\n",
    "plt.imshow(cv2.hconcat([image,mask_sq]))\n",
    "plt.show()\n",
    "# print(mask.shape)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(at_img.shape)\n",
    "mask[at_img==0] = 255\n",
    "mask = mask.astype(np.uint8)\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(mask,cmap='gray')\n",
    "plt.show()\n",
    "print(mask.shape)\n",
    "print(at_img.shape)\n",
    "\n",
    "# at_img_gray = cv2.cvtColor(at_img, cv2.COLOR_BGR2GRAY)\n",
    "print(at_img_gray.shape)\n",
    "mask_inv = 255 - mask\n",
    "fg = cv2.bitwise_and(at_img, at_img, mask=mask_inv)\n",
    "crop_image = image[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0],:]\n",
    "bg = cv2.bitwise_and(crop_image, crop_image, mask=mask)\n",
    "image[locate_h:locate_h+dsize[1], locate_w:locate_w+dsize[0],:] = fg+bg\n",
    "\n",
    "plt.imshow(fg)\n",
    "plt.show()\n",
    "plt.imshow(bg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "        A.RandomScale(scale_limit=(-0.6, -0.6), p=1), #LargeScaleJitter from scale of 0.1 to 2\n",
    "        A.PadIfNeeded(256, 256, border_mode=0), #pads with image in the center, not the top left like the paper\n",
    "        A.RandomCrop(256, 256),\n",
    "        CopyPaste(blend=True, sigma=1, pct_objects_paste=0.8, p=1., always_apply=True) #pct_objects_paste is a guess\n",
    "    ], bbox_params=A.BboxParams(format=\"pascal_voc\", min_visibility=0.05)\n",
    ")\n",
    "# blend=True,\n",
    "# sigma=3,\n",
    "# pct_objects_paste=0.1,\n",
    "# max_paste_objects=None,\n",
    "# p=0.5,\n",
    "# always_apply=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = A.Compose([\n",
    "        # A.RandomScale(scale_limit=(-0.9, 1), p=1), #LargeScaleJitter from scale of 0.1 to 2\n",
    "        # A.PadIfNeeded(256, 256, border_mode=0), #pads with image in the center, not the top left like the paper\n",
    "        # A.RandomCrop(256, 256),\n",
    "    ], bbox_params=A.BboxParams(format=\"coco\", min_visibility=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CocoDetectionCP(\n",
    "    './coco/train2014/', \n",
    "    './coco/annotations/instances_train2014.json', \n",
    "    transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = FigaroDataset(transform2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "impath = glob.glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/src/*.jpg')\n",
    "maskpath = glob.glob('/home/ubuntu/workspace/U-2-Net_portrait_sketch/Figaro1k/train/mask/*.pbm')\n",
    "\n",
    "path = impath[1]\n",
    "mask_path = maskpath[1]\n",
    "\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "mask = np.array(mask)\n",
    "\n",
    "obj_ids = np.unique(mask)\n",
    "obj_ids = obj_ids[1:]\n",
    "masks = mask == obj_ids[:, None, None]\n",
    "num_objs = len(obj_ids)\n",
    "\n",
    "class_id = 1\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(masks[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin,ymin,xmax,ymax,class_id])\n",
    "\n",
    "# print(masks.shape)\n",
    "masks = [masks.squeeze(0)]\n",
    "\n",
    "output = {\n",
    "            'image': image,\n",
    "            'masks': masks,\n",
    "            'bboxes': boxes,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = FigaroDataset(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = data2[16]\n",
    "image = img_data['image']\n",
    "masks = img_data['masks']\n",
    "bboxes = img_data['bboxes']\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = CocoDetectionCP(\n",
    "    './coco/train2014/', \n",
    "    './coco/annotations/instances_train2014.json', \n",
    "    transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = data3[3]\n",
    "image = img_data['image']\n",
    "masks = img_data['masks']\n",
    "bboxes = img_data['bboxes']\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgpath = glob.glob('/mnt/vitasoft/kobaco/dataset/data_processing/kobaco_data/scene/**/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "cnt = 0\n",
    "for path in bgpath:\n",
    "    # print(cv2.imread(path).shape[0])\n",
    "    # print('/mnt/vitasoft/kobaco/dataset/scene1280/' + os.path.basename(path))\n",
    "    # print(path)\n",
    "    if cv2.imread(path).shape[0] == 720:\n",
    "        os.makedirs('/mnt/vitasoft/kobaco/dataset/scene1280/', exist_ok=True)\n",
    "        shutil.copy(path, '/mnt/vitasoft/kobaco/dataset/scene1280/' + str(cnt).zfill(4) + '.jpg')\n",
    "        cnt+=1\n",
    "    else:\n",
    "        print(\"1\")\n",
    "    if cnt >= 4999:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgpath2 = glob.glob('/mnt/vitasoft/kobaco/dataset/scene1280/*.jpg')\n",
    "len(bgpath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch171",
   "language": "python",
   "name": "pytorch171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
